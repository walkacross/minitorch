{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdccf9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pragma cling add_include_path(\"./libtorch/include\")\n",
    "#pragma cling add_include_path(\"./libtorch/include/torch/csrc/api/include\")\n",
    "#pragma cling add_library_path(\"./libtorch/lib\")\n",
    "#pragma cling load(\"libtorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ecf4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>\n",
    "#include <tuple>\n",
    "#include <string>\n",
    "#include <vector>\n",
    "#include <memory>\n",
    "#include <type_traits>\n",
    "#include <torch/torch.h>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d404340b",
   "metadata": {},
   "source": [
    "# 1 Overview of at::Tensor\n",
    "\n",
    "## 1.1 the raw input keeps a Continuous memory addresses\n",
    "> The std::vector::data() is an STL in C++ which returns a direct pointer to the memory array used internally by the vector to store its owned elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e942aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data vector 1d: \n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\n"
     ]
    }
   ],
   "source": [
    "std::vector<float> flatted_data_vector1d = {1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18};\n",
    "std::cout << \"data vector 1d: \\n\" << flatted_data_vector1d << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7169be79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x5556d00a69b0\n"
     ]
    }
   ],
   "source": [
    "float* float_ptr = flatted_data_vector1d.data();\n",
    "std::cout << float_ptr << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde7920d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "std::cout << *float_ptr << std::endl;\n",
    "std::cout << *(float_ptr++) << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7d00e4",
   "metadata": {},
   "source": [
    "## 1.2 create at::Tensor from from_blob function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc31403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch::TensorOptions options = torch::TensorOptions().dtype(torch::kFloat32).layout(torch::kStrided).device(torch::kCPU).requires_grad(false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38b5fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "at::Tensor tensor2d = at::from_blob(/*void**/flatted_data_vector1d.data(), /*IntArrayRef*/{3,6}, /*const TensorOptions&*/options);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ff0388a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1   2   3   4   5   6\n",
      "  7   8   9  10  11  12\n",
      " 13  14  15  16  17  18\n",
      "[ CPUFloatType{3,6} ]\n"
     ]
    }
   ],
   "source": [
    "std::cout << tensor2d << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b476032f",
   "metadata": {},
   "source": [
    "![tensor](./tensor_hierarchy.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6980d",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/ops/from_blob.h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb4ded4",
   "metadata": {},
   "source": [
    "~~~\n",
    "// https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/ops/from_blob.h\n",
    "class TORCH_API TensorMaker {\n",
    "  friend TensorMaker for_blob(void* data, IntArrayRef sizes) noexcept;\n",
    "\n",
    " public:\n",
    "  using ContextDeleter = DeleterFnPtr;\n",
    "\n",
    "  TensorMaker& strides(OptionalIntArrayRef value) noexcept {\n",
    "    strides_ = value;\n",
    "    return *this;\n",
    "  }\n",
    "\n",
    "  TensorMaker& storage_offset(optional<int64_t> value) noexcept {\n",
    "    storage_offset_ = value;\n",
    "    return *this;\n",
    "  }\n",
    "\n",
    "  TensorMaker& context(void* value, ContextDeleter deleter = nullptr) noexcept {\n",
    "    ctx_ = std::unique_ptr<void, ContextDeleter>{\n",
    "        value, deleter != nullptr ? deleter : detail::noopDelete};\n",
    "\n",
    "    return *this;\n",
    "  }\n",
    "\n",
    "\n",
    "  TensorMaker& options(TensorOptions value) noexcept {\n",
    "    opts_ = value;\n",
    "\n",
    "    return *this;\n",
    "  }\n",
    "\n",
    "  Tensor make_tensor();\n",
    "\n",
    " private:\n",
    "  explicit TensorMaker(void* data, IntArrayRef sizes) noexcept : data_{data}, sizes_{sizes} {}\n",
    "  std::size_t computeStorageSize() const noexcept;\n",
    "  DataPtr makeDataPtrFromContext() noexcept;\n",
    "  IntArrayRef makeTempSizes() const noexcept;\n",
    "\n",
    "  void* data_;\n",
    "  IntArrayRef sizes_;\n",
    "  OptionalIntArrayRef strides_{};\n",
    "  optional<int64_t> storage_offset_{};\n",
    "  std::function<void(void*)> deleter_{};\n",
    "  std::unique_ptr<void, ContextDeleter> ctx_{nullptr, detail::noopDelete};\n",
    "  optional<Device> device_{};\n",
    "  TensorOptions opts_{};\n",
    "};\n",
    "\n",
    "inline TensorMaker for_blob(void* data, IntArrayRef sizes) noexcept {\n",
    "  return TensorMaker{data, sizes};\n",
    "}\n",
    "\n",
    "inline Tensor from_blob(\n",
    "    void* data,\n",
    "    IntArrayRef sizes,\n",
    "    const TensorOptions& options = {}) {\n",
    "  return for_blob(data, sizes).options(options).make_tensor();\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "735a9dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "// the implementation is in https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/templates/Functions.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ed29c",
   "metadata": {},
   "source": [
    "~~~\n",
    "namespace at {\n",
    "\n",
    "Tensor TensorMaker::make_tensor() {\n",
    "   AutoDispatchBelowADInplaceOrView guard{}; // TODO: Remove.\n",
    "   tracer::impl::NoTracerDispatchMode tracer_guard{};\n",
    "\n",
    "   if (device_ == nullopt) {\n",
    "     device_ = globalContext().getDeviceFromPtr(data_, opts_.device().type());\n",
    "   }\n",
    "\n",
    "   std::size_t size_bytes = computeStorageSize();\n",
    "\n",
    "   DataPtr data_ptr{};\n",
    "   if (deleter_) {\n",
    "     data_ptr = makeDataPtrFromDeleter();\n",
    "   } else {\n",
    "     data_ptr = makeDataPtrFromContext();\n",
    "   }\n",
    "\n",
    "   Storage storage{Storage::use_byte_size_t{}, size_bytes, std::move(data_ptr)};\n",
    "\n",
    "   Tensor tensor = detail::make_tensor<TensorImpl>(std::move(storage), opts_.computeDispatchKey(), opts_.dtype());\n",
    "\n",
    "  TensorImpl* tensor_impl = tensor.unsafeGetTensorImpl();\n",
    "  if (strides_) {\n",
    "    tensor_impl->set_sizes_and_strides(sizes_, *strides_);\n",
    "  } else {\n",
    "    tensor_impl->set_sizes_contiguous(sizes_);\n",
    "  }\n",
    "  if (storage_offset_) {\n",
    "    tensor_impl->set_storage_offset(*storage_offset_);\n",
    "  }\n",
    "   return tensor;\n",
    " }\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a08e9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "// entry_point to create at::Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de160896",
   "metadata": {},
   "source": [
    "~~~\n",
    "// https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/templates/TensorBody.h\n",
    "\n",
    "namespace detail {\n",
    "// Helper creator for Tensor class which doesn't requires the users to pass\n",
    "// in an intrusive_ptr instead it just converts the argument passed to\n",
    "// requested intrusive_ptr type.\n",
    "\n",
    "template <typename T, typename... Args>\n",
    "Tensor make_tensor(Args&&... args) {\n",
    "  return Tensor(c10::make_intrusive<T>(std::forward<Args>(args)...));\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df495f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "at::TensorMaker tensor_maker = at::for_blob(/*void**/flatted_data_vector1d.data(), /*IntArrayRef*/{3,6});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d55c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "//std::size_t size_bytes = tensor_maker.computeStorageSize();\n",
    "//std::cout << \"size_bytes: \" << size_bytes << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "810264df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itemsize: 4\n"
     ]
    }
   ],
   "source": [
    "std::size_t itemsize = options.dtype().itemsize();\n",
    "std::cout << \"itemsize: \" << itemsize << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22440620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 6]\n",
      "[6, 1]\n",
      "4\n",
      "0\n",
      "storage_n_bytes: 72\n"
     ]
    }
   ],
   "source": [
    "at::IntArrayRef tensor_sizes = tensor2d.sizes();\n",
    "std::cout << tensor_sizes << std::endl;\n",
    "\n",
    "at::IntArrayRef tensor_strides = tensor2d.strides();\n",
    "std::cout << tensor_strides << std::endl;\n",
    "\n",
    "std::size_t itemsize = tensor2d.dtype().itemsize();\n",
    "std::cout << itemsize << std::endl;\n",
    "\n",
    "std::size_t storage_offset = tensor2d.storage_offset();\n",
    "std::cout << storage_offset << std::endl;\n",
    "\n",
    "auto size_bytes = at::detail::computeStorageNbytes(tensor_sizes, tensor_strides, itemsize);\n",
    "std::cout << \"storage_n_bytes: \" << size_bytes << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beacb161",
   "metadata": {},
   "source": [
    "PyTorch引入了一个叫做步伐（Stride）的概念，其本质上是逻辑索引的一个相对距离，表明当你从某个元素沿着某一维度移动一个元素时的距离。图中是一个二维矩阵，所以其Stride是个size为2的一维向量。当我们从左往右移动的时候（1.0 -> 1.1），由于这两个数在内存中紧挨着，所以我们只移动了一次，Stride在这一维度的值是1；当我们从上往下移动的时候（1.0 -> 2.0），在内存中这两个数之间隔着2个数，所以我们移动了3次，Stride在这一维度的值是3。最终对于这个Tensor来说，它的Stride是（3，1）\n",
    "![](logic_physics_mapping.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12886d3",
   "metadata": {},
   "source": [
    "## 1.3 create a at::Tensor from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd0a7740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data vector 1d: \n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\n"
     ]
    }
   ],
   "source": [
    "std::vector<float> raw_flatted_data_vector = {1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18};\n",
    "std::cout << \"data vector 1d: \\n\" << raw_flatted_data_vector << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11b99e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "// step1: create DataPtr\n",
    "// https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/ops/from_blob.h\n",
    "using DeleterFnPtr = void (*)(void*);\n",
    "using ContextDeleter = DeleterFnPtr;\n",
    "std::unique_ptr<void, ContextDeleter> ctx_{nullptr, at::detail::noopDelete};\n",
    "at::DataPtr data_ptr{};\n",
    "data_ptr = at::DataPtr{raw_flatted_data_vector.data(), ctx_.release(), ctx_.get_deleter(), options.device()};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f55b6ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "// step2: create storage\n",
    "// https://github.com/pytorch/pytorch/blob/master/c10/core/Storage.h\n",
    "c10::Storage storage{/*use_byte_size*/c10::Storage::use_byte_size_t{}, /*size_bytes*/size_bytes, /*data_ptr*/std::move(data_ptr)};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8da9537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "std::cout << storage.nbytes() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4040e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n"
     ]
    }
   ],
   "source": [
    "std::cout << options.computeDispatchKey() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11c066f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "// step3: create TensorImpl ptr\n",
    "// https://github.com/pytorch/pytorch/blob/master/c10/core/TensorImpl.h \n",
    "c10::intrusive_ptr<c10::TensorImpl> impl_ = c10::make_intrusive<c10::TensorImpl>(std::move(storage),options.computeDispatchKey(), options.dtype());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93b22994",
   "metadata": {},
   "outputs": [],
   "source": [
    "// step4: create at::Tensor\n",
    "// https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/templates/TensorBody.h\n",
    "at::Tensor a_tensor = at::Tensor(/*tensor_impl*/impl_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b754af7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ CPUFloatType{0} ]\n",
      "current sizes: [0]\n",
      "current stride: [1]\n"
     ]
    }
   ],
   "source": [
    "std::cout << a_tensor << std::endl;\n",
    "std::cout << \"current sizes: \" << a_tensor.sizes() << std::endl;\n",
    "std::cout << \"current stride: \" << a_tensor.strides() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa9b3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "at::TensorImpl* tensor_impl = a_tensor.unsafeGetTensorImpl();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11d376cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "c10::IntArrayRef target_size{{3,6}};\n",
    "tensor_impl->set_sizes_contiguous(target_size);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "209b0a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1   2   3   4   5   6\n",
      "  7   8   9  10  11  12\n",
      " 13  14  15  16  17  18\n",
      "[ CPUFloatType{3,6} ]\n",
      "current sizes: [3, 6]\n",
      "current stride: [6, 1]\n"
     ]
    }
   ],
   "source": [
    "std::cout << a_tensor << std::endl;\n",
    "std::cout << \"current sizes: \" << a_tensor.sizes() << std::endl;\n",
    "std::cout << \"current stride: \" << a_tensor.strides() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba70a9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "std::cout << a_tensor.unsafeGetTensorImpl()->version_counter().current_version() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4f3c6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "std::cout << a_tensor.use_count() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c599a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "at::Tensor test_tensor = a_tensor;\n",
    "std::cout << test_tensor.use_count() << std::endl;\n",
    "std::cout << a_tensor.use_count() << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e383e0",
   "metadata": {},
   "source": [
    "## 1.4 what happens when reshape and slice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad936a5",
   "metadata": {},
   "source": [
    "当我们有了Stride后就可以快速的计算出一个元素的物理地址。熟悉C/C++的朋友会发现这其实和C/C++里的多级指针寻址方式非常类似。当我们在Tensor中查找索引是[1，1]的数时，我们通过计算索引和Stride的点积就可以得到逻辑表示的相对位置。而此元数的物理地址就是Tensor的第一个元素地址加上逻辑相对位置与字节数的乘积。最终我们计算出Tensor [1,1] 的物理地址是0x3f, 值是2.1。当然Stride在PyTorch里不仅仅是用来为某一个特定元素寻址。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6148fbdf",
   "metadata": {},
   "source": [
    "前面也说了Tensor是支持Python的切片操作，所以我们经常会有个需求是查看当前Tensor里的部分元素，当我们使用切片操作时就会获得一个新的Tensor。如果我们为每个切片操作都开辟一个新的内存空间，那势必会降低程序的运行效率以及造成内存空间浪费，所以PyTorch引入了一个叫做视图的概念，所有的视图都会共享相同的内存空间，而Stride就是创建新Tesnor视图的关键所在。当我们想要访问Tensor[0,1:] 时，我们创建了一个包含2个元素的一维Tensor视图，这两个元数在内存中是相邻的，所以其Stride是1。但当我们想要访问Tensor[:, 0] 时,我们虽然也创建了一个包含2个元素的一维Tensor，但是这两个数在内存中却相隔2个元素，所以其Stride是3. 正是由于Stride的存在，PyTorch才可以便捷地使所有Tensor视图共享一块内存空间。那PyTorch是怎么做到这些的呢？\n",
    "![](share_same_memory.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7968ba65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x5556c9d5fca0\n"
     ]
    }
   ],
   "source": [
    "std::cout << tensor_impl->data() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c49ece3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "at::Tensor another_tensor_from_reshape = a_tensor.reshape({6,3});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f7b83e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1   2   3\n",
      "  4   5   6\n",
      "  7   8   9\n",
      " 10  11  12\n",
      " 13  14  15\n",
      " 16  17  18\n",
      "[ CPUFloatType{6,3} ]\n",
      "current sizes: [6, 3]\n",
      "current stride: [3, 1]\n"
     ]
    }
   ],
   "source": [
    "std::cout << another_tensor_from_reshape << std::endl;\n",
    "std::cout << \"current sizes: \" << another_tensor_from_reshape.sizes() << std::endl;\n",
    "std::cout << \"current stride: \" << another_tensor_from_reshape.strides() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb7c47ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x5556c9d5fca0\n"
     ]
    }
   ],
   "source": [
    "std::cout << another_tensor_from_reshape.unsafeGetTensorImpl()->data() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "815917f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Tensor is a \"generic\" object holding a pointer to the underlying TensorImpl object, which\n",
    "// has an embedded reference count. In this way, Tensor is similar to boost::intrusive_ptr.\n",
    "//\n",
    "// For example:\n",
    "//\n",
    "// void func(Tensor a) {\n",
    "//   Tensor b = a;\n",
    "//   ...\n",
    "// }\n",
    "//\n",
    "// In this example, when we say Tensor b = a, we are creating a new object that points to the\n",
    "// same underlying TensorImpl, and bumps its reference count. When b goes out of scope, the\n",
    "// destructor decrements the reference count by calling release() on the TensorImpl it points to.\n",
    "// The existing constructors, operator overloads, etc. take care to implement the correct semantics.\n",
    "//\n",
    "// Note that Tensor can also be NULL, i.e. it is not associated with any underlying TensorImpl, and\n",
    "// special care must be taken to handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c37e8b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1   2   3   4   5   6\n",
      "  7   8   9  10  11  12\n",
      "[ CPUFloatType{2,6} ]\n",
      "current sizes: [2, 6]\n",
      "current stride: [6, 1]\n",
      "0x5556c9d5fca0\n"
     ]
    }
   ],
   "source": [
    "using torch::indexing::Slice;\n",
    "using torch::indexing::None;\n",
    "at::Tensor tensor_from_slice = a_tensor.index({Slice(None,2),Slice()});\n",
    "std::cout << tensor_from_slice << std::endl;\n",
    "std::cout << \"current sizes: \" << tensor_from_slice.sizes() << std::endl;\n",
    "std::cout << \"current stride: \" << tensor_from_slice.strides() << std::endl;\n",
    "std::cout << tensor_from_slice.unsafeGetTensorImpl()->data() << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65782a44",
   "metadata": {},
   "source": [
    "# 2 how to slice a tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c24737",
   "metadata": {},
   "source": [
    "## 2.1 the underlying method in at::Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d6aebee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1   2   3   4   5   6\n",
      "  7   8   9  10  11  12\n",
      " 13  14  15  16  17  18\n",
      "[ CPUFloatType{3,6} ]\n"
     ]
    }
   ],
   "source": [
    "std::cout << a_tensor << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff3d736e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1   2   3   4   5   6\n",
      "  7   8   9  10  11  12\n",
      "[ CPUFloatType{2,6} ]\n"
     ]
    }
   ],
   "source": [
    "std::cout << a_tensor.slice(/*dim*/0, /*start*/0, /*end*/2, /*step*/1) << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11adbb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1   2   3\n",
      "  7   8   9\n",
      " 13  14  15\n",
      "[ CPUFloatType{3,3} ]\n"
     ]
    }
   ],
   "source": [
    "std::cout << a_tensor.slice(/*dim*/1, /*start*/0, /*end*/3, /*step*/1) << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c0e8da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1   3   5\n",
      "  7   9  11\n",
      " 13  15  17\n",
      "[ CPUFloatType{3,3} ]\n"
     ]
    }
   ],
   "source": [
    "std::cout << a_tensor.slice(/*dim*/1, /*start*/0, /*end*/5, /*step*/2) << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7379f4",
   "metadata": {},
   "source": [
    "## 2.2 high-level wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48f405a",
   "metadata": {},
   "source": [
    "~~~\n",
    "// https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/TensorIndexing.cpp\n",
    "Tensor Tensor::index(ArrayRef<at::indexing::TensorIndex> indices) const {\n",
    "  TORCH_CHECK(indices.size() > 0, \"Passing an empty index list to Tensor::index() is not valid syntax\");\n",
    "  OptionalDeviceGuard device_guard(device_of(*this));\n",
    "  return at::indexing::get_item(*this, indices);\n",
    "}\n",
    "Tensor Tensor::index(std::initializer_list<at::indexing::TensorIndex> indices) const {\n",
    "  return index(ArrayRef<at::indexing::TensorIndex>(indices));\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863aa27",
   "metadata": {},
   "source": [
    "~~~\n",
    "//https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/TensorIndexing.h\n",
    "static inline Tensor get_item(\n",
    "    const Tensor& self,\n",
    "    const ArrayRef<TensorIndex>& indices,\n",
    "    bool disable_slice_optimization = false) {\n",
    "  at::Device self_device = self.device();\n",
    "  // NOTE [nested tensor size for indexing]\n",
    "  // nested tensor does not have a size (yet) so for now we represent its size\n",
    "  // as null may need to be changed after we reach a better solution for nested\n",
    "  // tensor size\n",
    "  c10::optional<SymIntArrayRef> self_sizes = self.is_nested()\n",
    "      ? c10::optional<SymIntArrayRef>(c10::nullopt)\n",
    "      : c10::optional<SymIntArrayRef>(self.sym_sizes());\n",
    "\n",
    "  // handle simple types: integers, slices, none, ellipsis, bool\n",
    "  if (indices.size() == 1) {\n",
    "    const TensorIndex& index = indices[0];\n",
    "    if (index.is_integer()) {\n",
    "      return impl::applySelect(\n",
    "          self, 0, index.integer(), 0, self_device, self_sizes);\n",
    "    } else if (index.is_slice()) {\n",
    "      return impl::applySlice(\n",
    "          self,\n",
    "          0,\n",
    "          index.slice().start(),\n",
    "          index.slice().stop(),\n",
    "          index.slice().step(),\n",
    "          /*disable_slice_optimization=*/true,\n",
    "          self_device,\n",
    "          self_sizes);\n",
    "    } else if (index.is_none()) {\n",
    "      return self.unsqueeze(0);\n",
    "    } else if (index.is_ellipsis()) {\n",
    "      return at::alias(self);\n",
    "    } else if (index.is_boolean()) {\n",
    "      Tensor result = self.unsqueeze(0);\n",
    "      return dispatch_index(\n",
    "          result,\n",
    "          std::vector<Tensor>{impl::boolToIndexingTensor(\n",
    "              result, index.boolean(), self_device)});\n",
    "    }\n",
    "  }\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe2e69",
   "metadata": {},
   "source": [
    "~~~\n",
    "static inline Tensor applySlice(\n",
    "    const Tensor& self,\n",
    "    int64_t dim,\n",
    "    int64_t start,\n",
    "    int64_t stop,\n",
    "    int64_t step,\n",
    "    bool disable_slice_optimization,\n",
    "    const at::Device& self_device,\n",
    "    const c10::optional<SymIntArrayRef>& self_sizes) {\n",
    "  // TODO: implement negative step\n",
    "  TORCH_CHECK_VALUE(step > 0, \"step must be greater than zero\");\n",
    "\n",
    "  // See NOTE [nested tensor size for indexing]\n",
    "  if (self_sizes.has_value()) {\n",
    "    // Skip this optimization if we are tracing, as the trace may be polymorphic\n",
    "    // over the shape of the `self` tensor, and we still want to record\n",
    "    // the slice.\n",
    "    SymInt length = (self_device == at::kCPU || self_device == at::kCUDA)\n",
    "        ? (*self_sizes)[dim]\n",
    "        : self.sym_size(dim);\n",
    "    if (!disable_slice_optimization && start == 0 && length == stop &&\n",
    "        step == 1) {\n",
    "      return self;\n",
    "    }\n",
    "  }\n",
    "  return self.slice(dim, start, stop, step);\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b5f52",
   "metadata": {},
   "source": [
    "## 2.3 how to implement Tensor.slice ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86436cf",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2788a172",
   "metadata": {},
   "source": [
    "ATen \"native\" functions are the modern mechanism for adding operators and functions to ATen. Native functions are declared in native_functions.yaml and have implementations defined in one of the cpp files in this directory.\n",
    "\n",
    "Like all ATen methods/functions, native functions are made available from both ATen's C++ and Python APIs. In C++, they are made available either as methods on Tensor (t.mymeth()) and functions in the ATen namespace (at::myfunc()). In PyTorch, they are made available as methods on Variable or as functions on torch._C._FunctionBase. (It is the user's responsibility to re-export these functions in a more user-facing module.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4801105e",
   "metadata": {},
   "source": [
    "### Registering a function in native_functions.yaml\n",
    "Every native function must have an entry in native_functions.yaml\n",
    "### the low_level operatipon of reshape is as_strided\n",
    "~~~\n",
    "- func: as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)\n",
    "  variants: function, method\n",
    "  dispatch:\n",
    "    ZeroTensor, CPU, CUDA: as_strided_tensorimpl\n",
    "    Meta: as_strided_tensorimpl_meta_symint\n",
    "    MPS: as_strided_tensorimpl_mps\n",
    "    QuantizedCPU, QuantizedCUDA: as_strided_qtensorimpl\n",
    "  device_check: NoCheck\n",
    "  device_guard: False\n",
    "  \n",
    "// https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/TensorShape.h\n",
    "\n",
    "Tensor as_strided_tensorimpl(const Tensor& self, IntArrayRef size, IntArrayRef stride, optional<int64_t> storage_offset_) {\n",
    "  auto storage_offset = storage_offset_.value_or(self.storage_offset());\n",
    "  auto result = at::detail::make_tensor<TensorImpl>(c10::TensorImpl::VIEW, Storage(self.storage()), self.key_set(), self.dtype());\n",
    "  setStrided(result, size, stride, storage_offset);\n",
    "  return result;\n",
    "}\n",
    "\n",
    "// https://github.com/pytorch/pytorch/aten/src/ATen/native/Resize.h\n",
    "\n",
    "template <typename T>\n",
    "inline void setStrided(\n",
    "    const Tensor& self,\n",
    "    ArrayRef<T> size,\n",
    "    ArrayRef<T> stride,\n",
    "    T storage_offset) {\n",
    "  TORCH_CHECK(size.size() == stride.size(), \"mismatch in length of strides and shape\");\n",
    "  auto* self_ = self.unsafeGetTensorImpl();\n",
    "  /* storage offset */\n",
    "  self_->set_sizes_and_strides(size, stride, c10::make_optional(storage_offset));\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47496da8",
   "metadata": {},
   "source": [
    "#### what happens when Tensor.permute()?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ed8e8",
   "metadata": {},
   "source": [
    "~~~\n",
    "https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/TensorShape.h\n",
    "\n",
    "Tensor permute(const Tensor& self, IntArrayRef dims) {\n",
    "  DimVector new_sizes, new_strides;\n",
    "  std::vector<int64_t> _;\n",
    "  std::tie(new_sizes, new_strides, _) = _permute_size_stride_estimation(self, dims);\n",
    "  return self.as_strided(new_sizes, new_strides);\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5183543d",
   "metadata": {},
   "source": [
    "#### what happens when Tensor.reshape()?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b5004a",
   "metadata": {},
   "source": [
    "~~~\n",
    "Tensor view(const Tensor& self,\n",
    "            at::IntArrayRef size) {\n",
    "  return view_impl(self, size);\n",
    "}\n",
    "\n",
    "Tensor view_impl(const Tensor& self, IntArrayRef size) {\n",
    "\n",
    "  at::DimVector inferred_size = at::infer_size_dv(size, self.numel());\n",
    "  auto stride = at::detail::computeStride(self.sizes(),\n",
    "                                          self.strides(),\n",
    "                                          inferred_size);\n",
    "  return alias_with_sizes_and_strides(self, inferred_size, *stride);\n",
    "}\n",
    "\n",
    "template <typename Vec>\n",
    "Tensor alias_with_sizes_and_strides(\n",
    "    const Tensor& self,\n",
    "    const Vec& sizes,\n",
    "    const Vec& strides) {\n",
    "  //caller should make sure that sizes and strides are valid for self\n",
    "  //(storage is sufficient, strides are non-negative, strides and sizes array size is the same)\n",
    "  Tensor self_;\n",
    "  if (self.is_quantized()) {\n",
    "    self_ = at::detail::make_tensor<QTensorImpl>(\n",
    "      c10::TensorImpl::VIEW, Storage(self.storage()), self.key_set(), self.dtype(), get_qtensorimpl(self)->quantizer());\n",
    "    auto* self_tmp_ = self_.unsafeGetTensorImpl();\n",
    "    self_tmp_->set_storage_offset(self.storage_offset());\n",
    "    self_tmp_->set_sizes_and_strides(sizes, strides);\n",
    "  } else {\n",
    "    self_ = at::detail::make_tensor<TensorImpl>(\n",
    "      c10::TensorImpl::VIEW, Storage(self.storage()), self.key_set(), self.dtype());\n",
    "    auto* self_tmp_ = self_.unsafeGetTensorImpl();\n",
    "    self_tmp_->set_storage_offset(self.storage_offset());\n",
    "    self_tmp_->set_sizes_and_strides(sizes, strides);\n",
    "  }\n",
    "  namedinference::propagate_names(self_, self);\n",
    "  return self_;\n",
    "}\n",
    "\n",
    "Tensor reshape(const Tensor& self, IntArrayRef proposed_shape) {\n",
    "  if (self.is_sparse()) {\n",
    "    AT_ERROR(\"reshape is not implemented for sparse tensors\");\n",
    "  }\n",
    "  DimVector shape = infer_size_dv(proposed_shape, self.numel());\n",
    "  \n",
    "  auto stride = at::detail::computeStride(self.sizes(), self.strides(), shape);\n",
    "  if (stride.has_value()) {\n",
    "    // Temporary check to revert to the old behavior/view in cases where the\n",
    "    // device is not supported (e.g. for XLA the operation is not supported\n",
    "    // so we use `view` instead).\n",
    "    //\n",
    "    // We need to do the checks here instead of in `native_functions.yaml`\n",
    "    // to preserve backwards compatibility.\n",
    "    if (!self.is_xla() && !self.is_lazy() && !self.is_ipu()) {\n",
    "      return self._reshape_alias(shape, stride.value());\n",
    "    } else {\n",
    "      return self.view(shape);\n",
    "    }\n",
    "  }\n",
    "  return at::_unsafe_view(self.clone(at::MemoryFormat::Contiguous), shape);\n",
    "}\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb4ae4b",
   "metadata": {},
   "source": [
    "#### understand Tensor.slice()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f74e9",
   "metadata": {},
   "source": [
    "~~~\n",
    "- func: slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)\n",
    "  variants: function, method\n",
    "  device_check: NoCheck\n",
    "  device_guard: False\n",
    "  dispatch:\n",
    "    CompositeExplicitAutograd: slice\n",
    "  tags: canonical\n",
    "\n",
    "// https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/TensorShape.h\n",
    "\n",
    "Tensor slice(\n",
    "    const Tensor& self,\n",
    "    int64_t dim,\n",
    "    c10::optional<int64_t> start,\n",
    "    c10::optional<int64_t> end,\n",
    "    int64_t step) {\n",
    "  int64_t ndim = self.dim();\n",
    "  if (ndim == 0) {\n",
    "    TORCH_CHECK_INDEX(false, \"slice() cannot be applied to a 0-dim tensor.\");\n",
    "  }\n",
    "  dim = maybe_wrap_dim(dim, ndim);\n",
    "  DimVector sizes(self.sizes().begin(), self.sizes().end());\n",
    "  DimVector strides(self.strides().begin(), self.strides().end());\n",
    "  // handle optional parameters\n",
    "  int64_t start_val = start.has_value() ? start.value() : 0;\n",
    "  int64_t end_val = end.has_value() ? end.value() : INT64_MAX;\n",
    "\n",
    "  // TODO: support negative strides\n",
    "  TORCH_CHECK(step > 0, \"slice step must be positive\");\n",
    "\n",
    "  if (start_val < 0) {\n",
    "    start_val += sizes[dim];\n",
    "  }\n",
    "  if (end_val < 0) {\n",
    "    end_val += sizes[dim];\n",
    "  }\n",
    "  if (start_val < 0) {\n",
    "    start_val = 0;\n",
    "  } else if (start_val >= sizes[dim]) {\n",
    "    start_val = sizes[dim];\n",
    "  }\n",
    "  if (end_val < start_val) {\n",
    "    end_val = start_val;\n",
    "  } else if (end_val >= sizes[dim]) {\n",
    "    end_val = sizes[dim];\n",
    "  }\n",
    "  auto storage_offset = self.storage_offset() + start_val * strides[dim];\n",
    "  auto len = end_val - start_val;\n",
    "  sizes[dim] = (len + step - 1) / step; // round-up\n",
    "  strides[dim] *= step;\n",
    "\n",
    "  Tensor result;\n",
    "  if (self.is_quantized()) {\n",
    "    auto quantizer = create_subtensor_quantizer(self, false, start_val, end_val, dim, step);\n",
    "    result = as_strided_qtensorimpl(self, sizes, strides, storage_offset, std::move(quantizer));\n",
    "  } else {\n",
    "    // NB: it is extremely important to perform a redispatch here for\n",
    "    // the MPS backend; if you call directly to as_strided_tensorimpl,\n",
    "    // the necessary metadata for MPS will not get setup and you will\n",
    "    // get silently wrong results\n",
    "    result = self.as_strided(sizes, strides, storage_offset);\n",
    "  }\n",
    "  namedinference::propagate_names(result, self);\n",
    "  return result;\n",
    "}\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34baee7a",
   "metadata": {},
   "source": [
    "## 2.5 how tensor operations?  if tensor share the same data pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b469bf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x5556c9d5fca0\n"
     ]
    }
   ],
   "source": [
    "std::cout << a_tensor.data_ptr() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff8c58a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x5556c9d5fca0\n"
     ]
    }
   ],
   "source": [
    "std::cout << tensor_from_slice.data_ptr() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8ffa74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1   2   3   4   5   6\n",
      "  7   8   9  10  11  12\n",
      "[ CPUFloatType{2,6} ]\n"
     ]
    }
   ],
   "source": [
    "std::cout << tensor_from_slice.data() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a3c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5581df2d",
   "metadata": {},
   "source": [
    "# reference\n",
    "\n",
    "* https://zhuanlan.zhihu.com/p/569278062"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77100992",
   "metadata": {},
   "source": [
    "~~~\n",
    "struct C10_API TensorImpl : public c10::intrusive_ptr_target {\n",
    "\n",
    " protected:\n",
    "  Storage storage_;\n",
    "  c10::VariableVersion version_counter_;\n",
    "\n",
    "  c10::impl::SizesAndStrides sizes_and_strides_;\n",
    "  int64_t storage_offset_ = 0;\n",
    "  int64_t numel_ = 1;\n",
    "\n",
    "\n",
    "  TensorImpl() = delete;\n",
    "  virtual ~TensorImpl() override;\n",
    "  \n",
    "  TensorImpl(\n",
    "      Storage&& storage,\n",
    "      DispatchKey dispatch_key,\n",
    "      const caffe2::TypeMeta data_type)\n",
    "      : TensorImpl(\n",
    "            std::move(storage),\n",
    "            DispatchKeySet(dispatch_key),\n",
    "            data_type) {}\n",
    "            \n",
    "  public:\n",
    "  TensorImpl(const TensorImpl&) = delete;\n",
    "  TensorImpl& operator=(const TensorImpl&) = delete;\n",
    "  TensorImpl(TensorImpl&&) = delete;\n",
    "  TensorImpl& operator=(TensorImpl&&) = delete;\n",
    "\n",
    "  IntArrayRef sizes() const {\n",
    "    return sizes_and_strides_.sizes_arrayref();\n",
    "  }\n",
    "\n",
    "  IntArrayRef strides() const {\n",
    "    return sizes_and_strides_.strides_arrayref();\n",
    "  }\n",
    "  \n",
    "  int64_t storage_offset() const {\n",
    "    return storage_offset_;\n",
    "  }\n",
    "  \n",
    "  int64_t numel() const {\n",
    "    return numel_;\n",
    "  }\n",
    "  \n",
    "  void refresh_numel() {\n",
    "      numel_ = c10::multiply_integers(sizes_and_strides_.sizes_arrayref());\n",
    "  }\n",
    "\n",
    "  void set_sizes_and_strides(\n",
    "      IntArrayRef new_size,\n",
    "      IntArrayRef new_stride,\n",
    "      c10::optional<int64_t> storage_offset = c10::nullopt) {\n",
    "\n",
    "    const auto new_dim = new_size.size();\n",
    "\n",
    "    sizes_and_strides_.set_sizes(new_size);\n",
    "\n",
    "    if (new_dim > 0) {\n",
    "      for (size_t dim = new_dim - 1;; dim--) {\n",
    "        if (new_stride[dim] >= 0) {\n",
    "          sizes_and_strides_.stride_at_unchecked(dim) = new_stride[dim];\n",
    "        } else {\n",
    "          // XXX: This behavior is surprising and may need to be removed to\n",
    "          // support negative strides. Some pytorch functions rely on it:\n",
    "          // for example, torch.cat (run TestTorch.test_cat_empty).\n",
    "          if (dim == new_dim - 1) {\n",
    "            sizes_and_strides_.stride_at_unchecked(dim) = 1;\n",
    "          } else {\n",
    "            // Keep stride monotonically increasing to match NumPy.\n",
    "            sizes_and_strides_.stride_at_unchecked(dim) =\n",
    "                std::max<int64_t>(\n",
    "                    sizes_and_strides_.size_at_unchecked(dim + 1), 1) *\n",
    "                sizes_and_strides_.stride_at_unchecked(dim + 1);\n",
    "          }\n",
    "        }\n",
    "        if (dim == 0)\n",
    "          break;\n",
    "      }\n",
    "    }\n",
    "\n",
    "    refresh_numel();\n",
    "    //refresh_contiguous();\n",
    "\n",
    "    if (storage_offset.has_value()) {\n",
    "      storage_offset_ = *storage_offset;\n",
    "    }\n",
    "  }\n",
    "  //\n",
    "  template <typename T>\n",
    "  inline T* data_ptr_impl() const {\n",
    "    // Caller does the type check.\n",
    "    return storage_.unsafe_data<T>() + storage_offset_;\n",
    "  }\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e83e1f7",
   "metadata": {},
   "source": [
    "~~~\n",
    "class TensorBase {\n",
    "\n",
    "protected:\n",
    "  c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl> impl_;\n",
    "  \n",
    "public:\n",
    "  TensorBase() = default;\n",
    "  // This constructor should not be used by end users and is an implementation\n",
    "  // detail invoked by autogenerated code.\n",
    "  explicit TensorBase(\n",
    "      c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl> tensor_impl)\n",
    "      : impl_(std::move(tensor_impl)) {\n",
    "    if (impl_.get() == nullptr) {\n",
    "      throw std::runtime_error(\"TensorImpl with nullptr is not supported\");\n",
    "    }\n",
    "  }\n",
    "  TensorBase(const TensorBase&) = default;\n",
    "  TensorBase(TensorBase&&) = default;\n",
    "  \n",
    "  TensorImpl* unsafeGetTensorImpl() const {\n",
    "    return impl_.get();\n",
    "  }\n",
    "  \n",
    "  void* data_ptr() const {\n",
    "    return this->unsafeGetTensorImpl()->data_ptr_impl();\n",
    "  }\n",
    "  \n",
    "  \n",
    "  int64_t numel() const {\n",
    "    return impl_->numel();\n",
    "  }\n",
    "  \n",
    "  IntArrayRef sizes() const {\n",
    "    return impl_->sizes();\n",
    "  }\n",
    "\n",
    "  IntArrayRef strides() const {\n",
    "    return impl_->strides();\n",
    "  }\n",
    "  \n",
    "  int64_t storage_offset() const {\n",
    "    return impl_->storage_offset();\n",
    "  }\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcddb1f5",
   "metadata": {},
   "source": [
    "# dispatcher in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a2b84cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <c10/core/DispatchKey.h>\n",
    "#include <c10/core/DispatchKeySet.h>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21efc63",
   "metadata": {},
   "source": [
    "### DispatchKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7fd4c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undefined\n"
     ]
    }
   ],
   "source": [
    "std::cout << c10::DispatchKey::Undefined << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6178acae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "std::cout << static_cast<std::uint16_t>(c10::DispatchKey::Undefined) << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d7ff7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "std::cout << static_cast<std::uint16_t>(c10::DispatchKey::CPU) << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "617053d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "std::cout << static_cast<std::uint16_t>(c10::DispatchKey::CUDA) << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af438662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "std::cout << static_cast<std::uint16_t>(c10::DispatchKey::Autocast) << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3093e80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "std::cout << static_cast<std::uint16_t>(c10::DispatchKey::AutocastCPU) << std::endl;\n",
    "std::cout << static_cast<std::uint16_t>(c10::DispatchKey::AutocastCUDA) << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b5c8597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "40\n",
      "41\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "std::cout << static_cast<std::uint16_t>(c10::DispatchKey::Autograd) << std::endl;\n",
    "std::cout << static_cast<std::uint16_t>(c10::DispatchKey::AutogradCPU) << std::endl;\n",
    "std::cout << static_cast<std::uint16_t>(c10::DispatchKey::AutogradCUDA) << std::endl;\n",
    "std::cout << static_cast<std::uint16_t>(c10::DispatchKey::ADInplaceOrView) << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc89995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "//uint8_t num_functionality_keys = static_cast<uint8_t>(c10::DispatchKey::EndOfFunctionalityKeys);\n",
    "//std::cout << num_functionality_keys << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1990ad1",
   "metadata": {},
   "source": [
    "### DispatchKeySet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92f9f867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data vector 1d: \n",
      "1 2 3 4 5 6\n"
     ]
    }
   ],
   "source": [
    "std::vector<float> data_vector1d = {1,2,3,4,5,6};\n",
    "std::cout << \"data vector 1d: \\n\" << data_vector1d << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63f8cfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor from vector1d: \n",
      " 1  2  3\n",
      " 4  5  6\n",
      "[ CPUFloatType{2,3} ]\n",
      "cpu\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "torch::Tensor tensor1 = torch::from_blob(data_vector1d.data(), {2,3});\n",
    "std::cout << \"tensor from vector1d: \\n\" << tensor1 << std::endl;\n",
    "std::cout << tensor1.device() << std::endl;\n",
    "std::cout << tensor1.requires_grad() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3de96016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DispatchKeySet(AutocastCPU, AutogradCPU, ADInplaceOrView, CPU)\n",
      "2252487008452609\n",
      "AutocastCPU\n"
     ]
    }
   ],
   "source": [
    "c10::DispatchKeySet dks1 = tensor1.key_set();\n",
    "std::cout << dks1 << std::endl;\n",
    "std::cout << dks1.raw_repr() << std::endl;\n",
    "c10::DispatchKey dk1 = dks1.highestPriorityTypeId();\n",
    "std::cout << dk1 << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a9be585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1  2  3\n",
      " 4  5  6\n",
      "[ CUDAFloatType{2,3} ]\n",
      "cuda:0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "auto options2 = torch::TensorOptions().dtype(torch::kFloat32).layout(torch::kStrided).device(torch::kCUDA).requires_grad(false);\n",
    "torch::Tensor tensor2= tensor1.to(options2);\n",
    "std::cout << tensor2 << std::endl;\n",
    "std::cout << tensor2.device() << std::endl;\n",
    "std::cout << tensor2.requires_grad() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b379abaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DispatchKeySet(Autocast, AutogradCUDA, ADInplaceOrView, CUDA)\n",
      "4504836577951746\n",
      "Autocast\n"
     ]
    }
   ],
   "source": [
    "c10::DispatchKeySet dks2 = tensor2.key_set();\n",
    "std::cout << dks2 << std::endl;\n",
    "std::cout << dks2.raw_repr() << std::endl;\n",
    "c10::DispatchKey dk2 = dks2.highestPriorityTypeId();\n",
    "std::cout << dk2 << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e28c5781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2325  0.7564 -0.4565\n",
      " 0.8446 -1.5105  0.3972\n",
      "[ CPUFloatType{2,3} ]\n",
      "cpu\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "auto options3 = torch::TensorOptions().dtype(torch::kFloat32).layout(torch::kStrided).device(torch::kCPU).requires_grad(true);\n",
    "torch::Tensor tensor3= torch::randn({2,3},options3);\n",
    "std::cout << tensor3 << std::endl;\n",
    "std::cout << tensor3.device() << std::endl;\n",
    "std::cout << tensor3.requires_grad() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb48da0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DispatchKeySet(AutocastCPU, AutogradCPU, ADInplaceOrView, CPU)\n",
      "2252487008452609\n",
      "AutocastCPU\n"
     ]
    }
   ],
   "source": [
    "c10::DispatchKeySet dks3 = tensor3.key_set();\n",
    "std::cout << dks3 << std::endl;\n",
    "std::cout << dks3.raw_repr() << std::endl;\n",
    "c10::DispatchKey dk3 = dks3.highestPriorityTypeId();\n",
    "std::cout << dk3 << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c46ea",
   "metadata": {},
   "source": [
    "#### custom DispatchKeySet constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "725a7e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "c10::DispatchKeySet custom_dks{{c10::DispatchKey::AutocastCPU,c10::DispatchKey::AutogradCPU, c10::DispatchKey::CPU}};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4344cf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DispatchKeySet(AutocastCPU, AutogradCPU, CPU)\n",
      "2252349569499137\n",
      "AutocastCPU\n"
     ]
    }
   ],
   "source": [
    "std::cout << custom_dks << std::endl;\n",
    "std::cout << custom_dks.raw_repr() << std::endl;\n",
    "c10::DispatchKey custom_dk = dks3.highestPriorityTypeId();\n",
    "std::cout << custom_dk << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863e56dc",
   "metadata": {},
   "source": [
    "# dtype in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "11607bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <c10/core/ScalarType.h>\n",
    "#include <c10/util/typeid.h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fc7dd987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int\n",
      "Long\n",
      "Float\n",
      "Double\n"
     ]
    }
   ],
   "source": [
    "std::cout << c10::ScalarType::Int << std::endl;\n",
    "std::cout << c10::ScalarType::Long << std::endl;\n",
    "std::cout << c10::ScalarType::Float << std::endl;\n",
    "std::cout << c10::ScalarType::Double << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "538875c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n",
      "4\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "std::cout << c10::elementSize(c10::ScalarType::Int) << std::endl;\n",
    "std::cout << c10::elementSize(c10::ScalarType::Long) << std::endl;\n",
    "std::cout << c10::elementSize(c10::ScalarType::Float) << std::endl;\n",
    "std::cout << c10::elementSize(c10::ScalarType::Double) << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "220f162f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float\n",
      "float\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "caffe2::TypeMeta float_dtype = caffe2::TypeMeta::Make<float>();\n",
    "std::cout << float_dtype << std::endl;\n",
    "std::cout << float_dtype.name() << std::endl;\n",
    "std::cout << float_dtype.itemsize() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01381e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int\n",
      "int\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "caffe2::TypeMeta int_dtype = caffe2::TypeMeta::Make<int>();\n",
    "std::cout << int_dtype << std::endl;\n",
    "std::cout << int_dtype.name() << std::endl;\n",
    "std::cout << int_dtype.itemsize() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "195532b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float\n"
     ]
    }
   ],
   "source": [
    "c10::ScalarType scalar_type = float_dtype.toScalarType();\n",
    "std::cout << scalar_type << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d4e1eb",
   "metadata": {},
   "source": [
    "#### std::optinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fe15a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <optional>\n",
    "#include <string>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "495ccac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "std::optional<std::string> a(\"yujiang\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "95f2536e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "std::cout << a.has_value() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5cab1190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yujiang\n"
     ]
    }
   ],
   "source": [
    "std::cout << a.value() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7296c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "std::optional<std::string> b = std::nullopt;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "697cd229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "std::cout << b.has_value() << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d8765971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing\n"
     ]
    }
   ],
   "source": [
    "std::cout << b.value_or(\"nothing\") << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112dea4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++17",
   "name": "xcpp17"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
